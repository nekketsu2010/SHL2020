{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Yu Gothic')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(kind, hold_position):\n",
    "    file_path = \"../Output/\" + kind + \"/\" + kind + \"_\" + hold_position + \"/\" + kind + \"_\" + hold_position\n",
    "    xy_mean = np.load(file_path + \"_glo_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_glo_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_glo_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_glo_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_glo_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_glo_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_glo_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_glo_gyro_z_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_glo_mag_norm_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    predict1 = np.load(file_path + \"_spectram_predict_0608.npy\")\n",
    "    if kind == \"train\":\n",
    "        user = [1] * 195491\n",
    "    else:\n",
    "        user = [2] * 14813 + [3] * 13872\n",
    "    user = np.array(user).reshape([-1, 1])\n",
    "#     predict2 = np.load(file_path + \"_spectram_predict_0527.npy\")\n",
    "#     predict3 = np.load(file_path + \"_spectram_predict_0528.npy\")\n",
    "    z_lacc_FFT = np.load(file_path + \"_glo_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "#     xy_gyro_FFT = np.load(file_path + \"_glo_gyro_xy_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_glo_gyro_z_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_glo_mag_norm_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, predict1, user, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.delete(np.load(\"../Data/センサ別npyファイル/train/train_Bag/train_Bag_Label.npy\")[:, 0], 120845, 0).reshape([-1])\n",
    "\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_Bag = np.delete(load_npy(\"train\", \"Bag\"), 120845, 0)\n",
    "train_Hips = np.delete(load_npy(\"train\", \"Hips\"), 120845, 0)\n",
    "# train_Torso = np.delete(load_npy(\"train\", \"Torso\"), 120845, 0)\n",
    "# train_Hand = np.delete(load_npy(\"train\", \"Hand\"), 120845, 0)\n",
    "\n",
    "train_Hips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.concatenate([train_Bag[(Y_train == 1) | (Y_train == 5) | (Y_train == 6) | (Y_train == 7) | (Y_train == 8)],\\\n",
    "#                           train_Hips, \\\n",
    "#                           train_Torso[(Y_train == 1) | (Y_train == 5) | (Y_train == 6) | (Y_train == 7) | (Y_train == 8)],\\\n",
    "#                           train_Hand[(Y_train == 1) | (Y_train == 6) | (Y_train == 7) | (Y_train == 8)]], axis=0)\n",
    "# X_train = np.concatenate([train_Bag, train_Hips, train_Torso], axis=0)\n",
    "X_train = train_Hips\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train = np.concatenate([Y_train[(Y_train == 1) | (Y_train == 5) | (Y_train == 6) | (Y_train == 7) | (Y_train == 8)],\\\n",
    "#                           Y_train, \\\n",
    "#                           Y_train[(Y_train == 1) | (Y_train == 5) | (Y_train == 6) | (Y_train == 7) | (Y_train == 8)], \\\n",
    "#                           Y_train[(Y_train == 1) | (Y_train == 6) | (Y_train == 7) | (Y_train == 8)]], axis=0)\n",
    "Y_train = Y_train.reshape([-1, 1])\n",
    "# Y_train = np.concatenate([Y_train, Y_train, Y_train], axis=0)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_Bag = load_npy(\"validation\", \"Bag\")\n",
    "validation_Hips = load_npy(\"validation\", \"Hips\")\n",
    "# validation_Torso = load_npy(\"validation\", \"Torso\")\n",
    "# validation_Hand = load_npy(\"validation\", \"Hand\")\n",
    "\n",
    "validation_Hips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = np.concatenate([validation_Bag, validation_Hips, validation_Torso, validation_Hand], axis=0)\n",
    "# X_val = np.concatenate([validation_Bag, validation_Hips, validation_Torso], axis=0)\n",
    "val = validation_Hips\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label = np.load(\"../Data/センサ別npyファイル/validation/validation_Bag/validation_Bag_Label.npy\")[:, 0].reshape([-1, 1])\n",
    "val_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy():\n",
    "    file_path = \"../Output/test/test\"\n",
    "    xy_mean = np.load(file_path + \"_glo_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_glo_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_glo_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_glo_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_glo_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_glo_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_glo_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_glo_gyro_z_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_glo_mag_norm_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    predict1 = np.load(file_path + \"_spectram_predict_0608.npy\").reshape([-1, 1])\n",
    "    user = np.load(file_path + \"_user.npy\")\n",
    "#     predict2 = np.load(file_path + \"_spectram_predict_0527.npy\").reshape([-1, 1])\n",
    "#     predict3 = np.load(file_path + \"_spectram_predict_0528.npy\")\n",
    "    z_lacc_FFT = np.load(file_path + \"_glo_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "#     xy_gyro_FFT = np.load(file_path + \"_glo_gyro_xy_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_glo_gyro_z_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_glo_mag_norm_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, predict1, user, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, predict1, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_npy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std = StandardScaler()\n",
    "\n",
    "X_train[:, :39] = train_std.fit_transform(X_train[:, :39])\n",
    "X_train[:, [i for i in range(40, X_train.shape[1], 2)]] = train_std.fit_transform(X_train[:, [i for i in range(40, X_train.shape[1], 2)]])\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user23_std = StandardScaler()\n",
    "\n",
    "user23 = np.concatenate([X_val, test], axis=0)\n",
    "\n",
    "user23_std.fit(user23[:, :39])\n",
    "X_val[:, :39] = user23_std.transform(X_val[:, :39])\n",
    "test[:, :39] = user23_std.transform(test[:, :39])\n",
    "\n",
    "user23_std.fit(user23[:, 41:user23.shape[1]:2])\n",
    "X_val[:, 41:X_val.shape[1]:2] = user23_std.fit_transform(X_val[:, 41:X_val.shape[1]:2])\n",
    "test[:, 41:test.shape[1]:2] = user23_std.fit_transform(test[:, 41:test.shape[1]:2])\n",
    "\n",
    "X_val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユーザごとに標準化\n",
    "### 標準化をvalidationデータとtestデータをまとめて行なうということ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータの標準化は同じ\n",
    "train_std = StandardScaler()\n",
    "X_train[:, :39] = train_std.fit_transform(X_train[:, :39])\n",
    "X_train[:, [i for i in range(41, X_train.shape[1], 2)]] = train_std.fit_transform(X_train[:, [i for i in range(41, X_train.shape[1], 2)]])\n",
    "\n",
    "# user2とuser3でまとめて標準化して、パラメータをとっておけば良い\n",
    "user2_1_std = StandardScaler()\n",
    "user2_2_std = StandardScaler()\n",
    "user2 = np.concatenate([val[val[:, 40] == 2].copy(), test[test[:, 40] == 2].copy()], axis=0)\n",
    "user2_1_std.fit(user2[:, :39])\n",
    "user2_2_std.fit(user2[:, [i for i in range(41, user2.shape[1], 2)]])\n",
    "user3_1_std = StandardScaler()\n",
    "user3_2_std = StandardScaler()\n",
    "user3 = np.concatenate([val[val[:, 40] == 3].copy(), test[test[:, 40] == 3].copy()], axis=0)\n",
    "user3_1_std.fit(user3[:, :39])\n",
    "user3_2_std.fit(user3[:, [i for i in range(41, user3.shape[1], 2)]])\n",
    "\n",
    "# 標準化\n",
    "val[val[:, 40] == 2, :39] = user2_1_std.transform(val[val[:, 40] == 2, :39])\n",
    "val[val[:, 40] == 2, 41:val.shape[1]:2] = user2_2_std.transform(val[val[:, 40] == 2, 41:val.shape[1]:2])\n",
    "\n",
    "test[test[:, 40] == 2, :39] = user2_1_std.transform(test[test[:, 40] == 2, :39])\n",
    "test[test[:, 40] == 2, 41:test.shape[1]:2] = user2_2_std.transform(test[test[:, 40] == 2, 41:test.shape[1]:2])\n",
    "\n",
    "val[val[:, 40] == 3, :39] = user3_1_std.transform(val[val[:, 40] == 3, :39])\n",
    "val[val[:, 40] == 3, 41:val.shape[1]:2] = user3_2_std.transform(val[val[:, 40] == 3, 41:val.shape[1]:2])\n",
    "\n",
    "test[test[:, 40] == 3, :39] = user3_1_std.transform(test[test[:, 40] == 3, :39])\n",
    "test[test[:, 40] == 3, 41:test.shape[1]:2] = user3_2_std.transform(test[test[:, 40] == 3, 41:test.shape[1]:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン2で学習して良いvalidationデータだけ取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_Bag = X_val[X_val.shape[0]//4*0:X_val.shape[0]//4]\n",
    "validation_Hips = X_val[X_val.shape[0]//4*1:X_val.shape[0]//4*2]\n",
    "validation_Torso = X_val[X_val.shape[0]//4*2:X_val.shape[0]//4*3]\n",
    "validation_Hand = X_val[X_val.shape[0]//4*3:]\n",
    "\n",
    "validation_Bag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_file = np.load(\"validation_pattern2.npy\").reshape([-1])\n",
    "\n",
    "# X_val = np.concatenate([validation_Bag[pattern_file == 1], validation_Hips[pattern_file == 1], validation_Torso[pattern_file == 1], validation_Hand[pattern_file == 1]], axis=0)\n",
    "X_val = validation_Hips[pattern_file == 1]\n",
    "Y_val = np.load(\"../Data/センサ別npyファイル/validation/validation_Bag/validation_Bag_Label.npy\")[:, 0].reshape([-1, 1])\n",
    "Y_val = Y_val.reshape([-1, 1])[pattern_file == 1]\n",
    "# Y_val = np.concatenate([Y_val, Y_val, Y_val, Y_val], axis=0)\n",
    "# print(Y_val.shape[0] // 4)\n",
    "# Y_val_hold_position = np.zeros((14338*4, 1))\n",
    "# for i in range(4):\n",
    "#     Y_val_hold_position[14338*i:14338*(i+1)] = i\n",
    "# Y_val = np.concatenate([Y_val, Y_val_hold_position], axis=1)\n",
    "\n",
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_file = np.load(\"validation_pattern2.npy\").reshape([-1])\n",
    "X_train = np.concatenate([X_train, val[pattern_file == 1]], axis=0)\n",
    "X_val = val[pattern_file == 0]\n",
    "\n",
    "Y_train = np.concatenate([Y_train, val_label[pattern_file == 1]], axis=0)\n",
    "Y_val = val_label[pattern_file == 0]\n",
    "\n",
    "X_test = val[pattern_file == 2]\n",
    "Y_test = val_label[pattern_file == 2]\n",
    "\n",
    "Y_train.shape, Y_val.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Still = X_train[(Y_train==1)[:, 0]]\n",
    "train_Walking = X_train[(Y_train==2)[:, 0]]\n",
    "train_Run = X_train[(Y_train==3)[:, 0]]\n",
    "train_Bike = X_train[(Y_train==4)[:, 0]]\n",
    "\n",
    "train_Still.shape, train_Walking.shape, train_Run.shape, train_Bike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_Still = train_Still[np.random.choice(train_Still.shape[0], 33700, replace=False)]\n",
    "train_Walking = train_Walking[np.random.choice(train_Walking.shape[0], 33700, replace=False)]\n",
    "train_Bike = train_Bike[np.random.choice(train_Bike.shape[0], 33700, replace=False)]\n",
    "\n",
    "train_Still.shape, train_Walking.shape, train_Bike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([train_Still, train_Walking, train_Run, train_Bike], axis=0)\n",
    "Y_train = [1] * 557016 + [2] * 33700 + [3] * 33700 + [4] * 33700\n",
    "Y_train = np.array(Y_train).reshape([-1, 1])\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evals_result = {}\n",
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, n_estimators=10000,\n",
    "                        n_jobs=-1, tree_method='gpu_hist', gpu_id=0)\n",
    "\n",
    "model.fit(x_train, y_train, early_stopping_rounds=30, eval_set=[(x_train, y_train), (x_test, y_test)], eval_metric='merror', verbose=False, callbacks=[xgb.callback.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evals_result = {}\n",
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, n_estimators=10000,\n",
    "                        n_jobs=-1, tree_method='gpu_hist', gpu_id=0)\n",
    "\n",
    "model.fit(X_train, Y_train, early_stopping_rounds=30, eval_set=[(X_train, Y_train), (X_val, Y_val)], eval_metric='merror', verbose=False, callbacks=[xgb.callback.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習過程の名前は 'validation_{n}' になる\n",
    "plt.figure(figsize=(24, 4))\n",
    "train_metric = evals_result['validation_0']['merror']\n",
    "plt.plot(train_metric, label='train merror')\n",
    "eval_metric = evals_result['validation_1']['merror']\n",
    "plt.plot(eval_metric, label='eval merror')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('rounds(epochs)')\n",
    "plt.ylabel('merror')\n",
    "plt.xticks(np.arange(0, 275+1, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y,pred_y,class_names,normalize=False, figsize=(16, 8)):\n",
    "    cm = confusion_matrix(test_y,pred_y)\n",
    "    # classes = class_names[unique_labels(test_y,pred_y)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           ylabel='True label\\n',\n",
    "           xlabel='\\nPredicted label')\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j,\n",
    "                    i,\n",
    "                    format(cm[i, j], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"red\", fontsize=24)\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = ['Other', 'Walking', 'Run', 'Bike']\n",
    "class_names = ['Still', 'Walking', 'Run', 'Bike', 'Car', 'Bus', 'Train', 'Subway']\n",
    "predict = model.predict(X_val)\n",
    "f1_macro = f1_score(Y_val, predict, average='macro')\n",
    "plot_confusion_matrix(Y_val, predict, class_names, False)\n",
    "plt.grid(False)\n",
    "round(f1_macro, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = ['Other', 'Walking', 'Run', 'Bike']\n",
    "class_names = ['Still', 'Walking', 'Run', 'Bike', 'Car', 'Bus', 'Train', 'Subway']\n",
    "predict = model.predict(X_val)\n",
    "\n",
    "for i in range(2, 4):\n",
    "    f1_macro = f1_score(Y_val[X_val[:,40] == i], predict[X_val[:,40] == i], average='macro')\n",
    "    plot_confusion_matrix(Y_val[X_val[:,40] == i], predict[X_val[:,40] == i], class_names, False)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"ユーザ\" + str(i))\n",
    "    print(\"ユーザ\" + str(i), round(f1_macro, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = ['Other', 'Walking', 'Run', 'Bike']\n",
    "class_names = ['Still', 'Walking', 'Run', 'Bike', 'Car', 'Bus', 'Train', 'Subway']\n",
    "predict = model.predict(X_test)\n",
    "f1_macro = f1_score(Y_test, predict, average='macro')\n",
    "plot_confusion_matrix(Y_test, predict, class_names, False)\n",
    "plt.grid(False)\n",
    "round(f1_macro, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = ['Other', 'Walking', 'Run', 'Bike']\n",
    "class_names = ['Still', 'Walking', 'Run', 'Bike', 'Car', 'Bus', 'Train', 'Subway']\n",
    "predict = model.predict(X_test)\n",
    "\n",
    "for i in range(2, 4):\n",
    "    f1_macro = f1_score(Y_test[X_test[:,40] == i], predict[X_test[:,40] == i], average='macro')\n",
    "    plot_confusion_matrix(Y_test[X_test[:,40] == i], predict[X_test[:,40] == i], class_names, False)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"ユーザ\" + str(i) + \"  F値：\" + str(round(f1_macro, 3)), fontsize=24)\n",
    "    print(\"ユーザ\" + str(i), round(f1_macro, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 24))\n",
    "xgb.plot_importance(model,\n",
    "                    ax=ax,\n",
    "                    importance_type='gain',\n",
    "                    show_values=False)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open(\"model_0606_user.binaryfile\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, file_name):\n",
    "    x_predict = model.predict_proba(x).reshape([-1, 8])\n",
    "    np.save(file_name, x_predict)\n",
    "    return x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Bag_predict = predict(X_train[X_train.shape[0]//4*0:X_train.shape[0]//4], \"train_Bag_関口分類\")\n",
    "train_Hips_predict = predict(X_train[X_train.shape[0]//4:X_train.shape[0]//4*2], \"train_Hips_関口分類\")\n",
    "train_Torso_predict = predict(X_train[X_train.shape[0]//4*2:X_train.shape[0]//4*3], \"train_Torso_関口分類\")\n",
    "train_Hand_predict = predict(X_train[X_train.shape[0]//4*3:], \"train_Hand_関口分類\")\n",
    "\n",
    "validation_Bag_predict = predict(X_val[X_val.shape[0]//4*0:X_val.shape[0]//4], \"validation_Bag_関口分類\")\n",
    "validation_Hips_predict = predict(X_val[X_val.shape[0]//4:X_val.shape[0]//4*2], \"validation_Hips_関口分類\")\n",
    "validation_Torso_predict = predict(X_val[X_val.shape[0]//4*2:X_val.shape[0]//4*3], \"validation_Torso_関口分類\")\n",
    "validation_Hand_predict = predict(X_val[X_val.shape[0]//4*3:], \"validation_Hand_関口分類\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Hips_predict = predict(X_train, \"train_Hips_関口分類\")\n",
    "validation_Hips_predict = predict(X_val, \"validation_Hips_関口分類\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン2のときこっち"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_Bag_predict = predict(X_train[X_train.shape[0]//4*0:X_train.shape[0]//4], \"train_Bag_関口分類\")\n",
    "# train_Hips_predict = predict(X_train[X_train.shape[0]//4:X_train.shape[0]//4*2], \"train_Hips_関口分類\")\n",
    "# train_Torso_predict = predict(X_train[X_train.shape[0]//4*2:X_train.shape[0]//4*3], \"train_Torso_関口分類\")\n",
    "# train_Hand_predict = predict(X_train[X_train.shape[0]//4*3:], \"train_Hand_関口分類\")\n",
    "\n",
    "train_Hips_predict = predict(X_train[X_train[:, 40] == 1], \"train_Hips_関口分類_pattern2\")\n",
    "\n",
    "# validation_Bag_predict = predict(validation_Bag, \"validation_Bag_関口分類\")\n",
    "validation_Hips_predict = predict(val, \"validation_Hips_関口分類_pattern2\")\n",
    "# validation_Torso_predict = predict(validation_Torso, \"validation_Torso_関口分類\")\n",
    "# validation_Hand_predict = predict(validation_Hand, \"validation_Hand_関口分類\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_f1_score = 0\n",
    "best_param = {}\n",
    "\n",
    "max_depth = [11, 12, 13]\n",
    "min_child_weight = [3, 4, 5]\n",
    "count = 1\n",
    "for i in max_depth:\n",
    "    for j in min_child_weight:\n",
    "        evals_result = {}\n",
    "        model = xgb.XGBClassifier(max_depth=i, min_child_weight=j, learning_rate=0.1,\n",
    "                                  n_jobs=-1, tree_method='gpu_hist', gpu_id=0)\n",
    "        model.fit(X_train, Y_train)\n",
    "#         model.fit(x_train, y_train, early_stopping_rounds=100, eval_set=[[(x_train, y_train), (x_test, y_test)]], eval_metric='merror', verbose_eval=False, callbacks=[xgb.callback.record_evaluation(evals_result)])\n",
    "        print(\"{}回終わった\".format(count))\n",
    "        count += 1\n",
    "        predict = model.predict(X_val)\n",
    "        f1_macro = f1_score(Y_val, predict, average='macro')\n",
    "        print({'max_depth': i, 'min_child_weight': j}, f1_macro)\n",
    "        if f1_macro > best_f1_score:\n",
    "            best_f1_score = f1_macro\n",
    "            best_param = {'max_depth': i, 'min_child_weight': j}\n",
    "\n",
    "print(round(best_f1_score, 3))\n",
    "print(best_param)\n",
    "\n",
    "# param = {\n",
    "#     \"max_depth\":[i for i in range(3, 16, 3)], \"min_child_weight\":[i for i in range(6, 13, 2)]\n",
    "#     # \"gamma\":[0.01, 0.05, 0.075, 0.1]\n",
    "#     # \"subsample\":[0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00], \"colsample_bytree\":[0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "#     # \"n_estimators\":[250, 500, 750], \"learning_rate\":[0.05, 0.1]\n",
    "#     # \"reg_alpha\":[0.1, 0.25, 0.5, 0.75]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習過程の名前は 'validation_{n}' になる\n",
    "train_metric = evals_result['validation_0']['merror']\n",
    "plt.plot(train_metric, label='train merror')\n",
    "eval_metric = evals_result['validation_1']['merror']\n",
    "plt.plot(eval_metric, label='eval merror')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('rounds(epochs)')\n",
    "plt.ylabel('merror')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testData推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict_proba(test)\n",
    "\n",
    "test_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where((test_predict[:, 1] >= 0.75) | (test_predict[:, 2] >= 0.75) | (test_predict[:, 4] >= 0.75))[0]\n",
    "# np.save(\"test_walking_run_car_index\", a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(test_predict[:, 1] >= 0.75)[0].shape, np.where(test_predict[:, 2] >= 0.75)[0].shape, np.where(test_predict[:, 4] >= 0.75)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where((test_predict[:, 2] >= 0.75))[0]\n",
    "np.save(\"test_run_index\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_関口分類_pattern2\", test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(test_predict >= 0.75)[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = "
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
