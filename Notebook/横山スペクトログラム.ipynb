{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from numpy.fft import fft\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Yu Gothic')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Data/センサ別npyファイル\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    x = np.sqrt(np.square(x[:, :, 0]) + np.square(x[:, :, 1]) + np.square(x[:, :, 2]))\n",
    "    return x.reshape([-1, 500])\n",
    "\n",
    "def spectrogtam(x):\n",
    "    nfft = 80\n",
    "    overlap = nfft - 10\n",
    "    \n",
    "    x = scipy.signal.spectrogram(x, fs=100, nfft=nfft, noverlap=overlap, nperseg=nfft)[2]\n",
    "    x = (x - x.mean(axis=1, keepdims=True)) / x.std(axis=1, keepdims=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195491,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = np.load(file_path + \"/train/train_Hips/train_Hips_Label.npy\")[:, 0].reshape([-1])\n",
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195491, 1, 41, 43, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_npy(sensor=\"Acc\", kind=\"train\"):\n",
    "    x = np.load(file_path + \"/\" + kind + \"/\" + kind + \"_Hips/\" + kind + \"_Hips_Glo_\" + sensor + \"_ver3.npy\")\n",
    "    x = norm(x)\n",
    "    x = np.apply_along_axis(spectrogtam, 1, x)\n",
    "    return x\n",
    "\n",
    "train_Hips_Acc = load_npy(\"Acc\", \"train\")\n",
    "train_Hips_Acc = train_Hips_Acc.reshape([-1, 1, train_Hips_Acc.shape[1], train_Hips_Acc.shape[2], 1])\n",
    "train_Hips_Mag = load_npy(\"Mag\", \"train\")\n",
    "train_Hips_Mag = train_Hips_Mag.reshape([-1, 1, train_Hips_Mag.shape[1], train_Hips_Mag.shape[2], 1])\n",
    "train_Hips_Gyr = load_npy(\"Gyr\", \"train\")\n",
    "train_Hips_Gyr = train_Hips_Gyr.reshape([-1, 1, train_Hips_Gyr.shape[1], train_Hips_Gyr.shape[2], 1])\n",
    "\n",
    "train_Hips_Acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([train_Hips_Acc, train_Hips_Mag, train_Hips_Gyr], axis=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_file = np.load(\"\").reshape([-1]) #パターンファイルのパス入れてくれ\n",
    "pattern_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label = np.load(file_path + \"/validation/validation_Hips/validation_Hips_Label.npy\")[:, 0].reshape([-1])\n",
    "val_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_Hips_Acc = load_npy(\"Acc\", \"val\").reshape([-1, 1, validation_Hips_Acc.shape[1], validation_Hips_Acc.shape[2], 1])\n",
    "validation_Hips_Mag = load_npy(\"Mag\", \"val\").reshape([-1, 1, validation_Hips_Mag.shape[1], validation_Hips_Mag.shape[2], 1])\n",
    "validation_Hips_Gyr = load_npy(\"Gyr\", \"val\").reshape([-1, 1, validation_Hips_Gyr.shape[1], validation_Hips_Gyr.shape[2], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.concatenate([val_Hips_Acc, val_Hips_Mag, val_Hips_Gyr], axis=1)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.round(X_train, 5)\n",
    "X_val = np.round(X_val, 5)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validationデータをパターンで分ける処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pattern0 = X_val[pattern_file == 0]\n",
    "X_val_pattern1 = X_val[pattern_file == 1]\n",
    "X_val_pattern2 = X_val[pattern_file == 2]\n",
    "\n",
    "X_val_pattern0.shape, X_val_pattern1.shape, X_val_pattern2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを7:3にするパターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateData(x, y):\n",
    "    pattern = np.zeros(x.shape[0])\n",
    "    for i in range(8):\n",
    "        tmp = np.where(y == (i+1))[0]\n",
    "        pattern[tmp[tmp.shape[0]//10*7:]] = 1\n",
    "        \n",
    "    x_train = x[pattern == 0]\n",
    "    x_test = x[pattern == 1]\n",
    "    y_train = y[pattern == 0]\n",
    "    y_test = y[pattern == 1]\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = separateData(X_train, train_label)\n",
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(input_shape):\n",
    "    input1 = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='valid', activation='relu', kernel_initializer=he_normal())(input1)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = models.Model(inputs=input1, outputs=x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = BuildModel(x_train[0, 0].shape)\n",
    "x2 = BuildModel(x_train[0, 1].shape)\n",
    "x3 = BuildModel(x_train[0, 2].shape)\n",
    "\n",
    "combined = layers.concatenate([x1.output, x2.output, x3.output])\n",
    "\n",
    "z = layers.Dense(64, activation='relu')(combined)\n",
    "z = layers.Dense(16, activation='relu')(z)\n",
    "z = layers.Dense(5, activation='softmax')(z)\n",
    "\n",
    "model = models.Model(inputs=[x1.input, x2.input, x3.input], outputs=z)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"/\" #保存ディレクトリを指定(後ろにスラッシュ入れてね)\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=save_folder + \"model_{epoch:02d}-{loss:.2f}-{accuracy:.2f}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5\", \n",
    "                                           monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='auto')\n",
    "history = model.fit([x_train[:, 0], x_train[:, 1], x_train[:, 2]], y_train, epochs=256, batch_size=512, \\\n",
    "                    validation_data=([x_val[:, 0], x_val[:, 1], x_val[:, 2]], y_val), callbacks=[cp_cb, es_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.ylim((0, 3.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(x1, x2, x3):\n",
    "    model = tf.keras.models.load_model(\"\") #ベストモデルのパス入れて！\n",
    "    predict = model.predict([x1, x2, x3])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npy(kind, hold_position, x):\n",
    "    np.save(kind + \"_\" + hold_position + \"_横山分類_pattern2\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(kind, hold_position):\n",
    "    label = np.load(file_path + kind + \"/\" + kind + \"_Hips/\" + kind + \"_Hips_Label.npy\")[:, 0].reshape([-1])\n",
    "    x1, x2, x3 = load_npy(\"Acc\", kind), load_npy(\"Mag\", kind), load_npy(\"Gyr\", kind)\n",
    "    x1 = x1.reshape([-1, x1.shape[1], x1.shape[2], x1.shape[3], 1])\n",
    "    x2 = x2.reshape([-1, x2.shape[1], x2.shape[2], x2.shape[3], 1])\n",
    "    x3 = x3.reshape([-1, x3.shape[1], x3.shape[2], x3.shape[3], 1])\n",
    "    x1 = np.round(x1, 5)\n",
    "    x2 = np.round(x2, 5)\n",
    "    x3 = np.round(x3, 5)\n",
    "    predict = predict_func(x1, x2, x3)\n",
    "    save_npy(kind, hold_position, predict.reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"train\", \"validation\"]\n",
    "hold_positions = [\"Hips\"]\n",
    "\n",
    "i = 0\n",
    "for kind in kinds:\n",
    "    for hold_position in tqdm(hold_positions):\n",
    "        flow(kind, hold_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testデータの推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load(sensor=\"Acc\"):\n",
    "    x = np.load(file_path + \"/test/test_Glo_\" sensor + \"_ver3.npy\")\n",
    "    x = norm(x)\n",
    "    x = spectrogtam(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1, test2, test3 = test_load(\"Acc\"), test_load(\"Mag\"), test_load(\"Gyr\")\n",
    "test1 = np.round(test1, 5)\n",
    "test2 = np.round(test2, 5)\n",
    "test3 = np.round(test3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([test1, test2, test3])\n",
    "np.save(\"test_横山分類_pattern2.npy\", predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
