{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from numpy.fft import fft\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Yu Gothic')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(hold_position):\n",
    "    file_path = \"../Output/validation/validation_\" + hold_position + \"/validation_\" + hold_position\n",
    "    xy_mean = np.load(file_path + \"_glo_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_glo_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_glo_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_glo_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_glo_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_glo_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT = np.load(file_path + \"_glo_laccel_z_amplitude_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT = np.load(file_path + \"_glo_gyro_z_amplitude_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), z_lacc_FFT, z_gyro_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_gyro_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(hold_position):\n",
    "    file_path = \"../Output/validation/validation_\" + hold_position + \"/validation_\" + hold_position\n",
    "    xy_mean = np.load(file_path + \"_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_gyro_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_mag_norm_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_lacc_FFT = np.load(file_path + \"_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_gyro_z_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_mag_norm_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = load_npy(\"Bag\")\n",
    "Hips = load_npy(\"Hips\")\n",
    "Torso = load_npy(\"Torso\")\n",
    "Hand = load_npy(\"Hand\")\n",
    "\n",
    "Hand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.load(\"../Data/センサ別npyファイル/validation/validation_Bag/validation_Bag_Label.npy\")[:, 0].reshape([-1, 1])\n",
    "\n",
    "user2_Label = label[:14813]\n",
    "user3_Label = label[14813:]\n",
    "\n",
    "user2_Label.shape, user3_Label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2_Bag = Bag[:14813]\n",
    "user2_Hips = Hips[:14813]\n",
    "user2_Torso = Torso[:14813]\n",
    "user2_Hand = Hand[:14813]\n",
    "\n",
    "user3_Bag = Bag[14813:]\n",
    "user3_Hips = Hips[14813:]\n",
    "user3_Torso = Torso[14813:]\n",
    "user3_Hand = Hand[14813:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2_Bag_train, user2_Bag_val = train_test_split(user2_Bag, test_size=0.3, shuffle=False)\n",
    "user2_Hips_train, user2_Hips_val = train_test_split(user2_Hips, test_size=0.3, shuffle=False)\n",
    "user2_Torso_train, user2_Torso_val = train_test_split(user2_Torso, test_size=0.3, shuffle=False)\n",
    "user2_Hand_train, user2_Hand_val = train_test_split(user2_Hand, test_size=0.3, shuffle=False)\n",
    "\n",
    "user3_Bag_train, user3_Bag_val = train_test_split(user3_Bag, test_size=0.3, shuffle=False)\n",
    "user3_Hips_train, user3_Hips_val = train_test_split(user3_Hips, test_size=0.3, shuffle=False)\n",
    "user3_Torso_train, user3_Torso_val = train_test_split(user3_Torso, test_size=0.3, shuffle=False)\n",
    "user3_Hand_train, user3_Hand_val = train_test_split(user3_Hand, test_size=0.3, shuffle=False)\n",
    "\n",
    "user2_Label_train, user2_Label_val = train_test_split(user2_Label, test_size=0.3, shuffle=False)\n",
    "user3_Label_train, user3_Label_val = train_test_split(user3_Label, test_size=0.3, shuffle=False)\n",
    "\n",
    "user2_Hand_train.shape, user2_Hand_val.shape, user3_Hand_train.shape, user3_Hand_val.shape, user2_Label_train.shape, user2_Label_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy():\n",
    "    file_path = \"../Output/test/test\"\n",
    "    xy_mean = np.load(file_path + \"_glo_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_glo_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_glo_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_glo_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_glo_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_glo_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT = np.load(file_path + \"_glo_laccel_z_amplitude_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT = np.load(file_path + \"_glo_gyro_z_amplitude_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), z_lacc_FFT, z_gyro_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_gyro_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy():\n",
    "    file_path = \"../Output/test/test\"    \n",
    "    xy_mean = np.load(file_path + \"_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_gyro_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_mag_norm_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_lacc_FFT = np.load(file_path + \"_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_gyro_z_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_mag_norm_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_npy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [0] * user2_Bag_train.shape[0] + [1] * user2_Bag_train.shape[0] + [2] * user2_Bag_train.shape[0] + [3] * user2_Bag_train.shape[0] + \\\n",
    "        [0] * user3_Bag_train.shape[0] + [1] * user3_Bag_train.shape[0] + [2] * user3_Bag_train.shape[0] + [3] * user3_Bag_train.shape[0]\n",
    "Y_train = np.array(Y_train).reshape([-1, 1])\n",
    "\n",
    "Y_val = [0] * user2_Bag_val.shape[0] + [1] * user2_Bag_val.shape[0] + [2] * user2_Bag_val.shape[0] + [3] * user2_Bag_val.shape[0] + \\\n",
    "        [0] * user3_Bag_val.shape[0] + [1] * user3_Bag_val.shape[0] + [2] * user3_Bag_val.shape[0] + [3] * user3_Bag_val.shape[0]\n",
    "Y_val = np.array(Y_val).reshape([-1, 1])\n",
    "\n",
    "Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2_Label_train = np.concatenate([user2_Label_train, user2_Label_train, user2_Label_train, user2_Label_train], axis=0)\n",
    "user2_Label_val = np.concatenate([user2_Label_val, user2_Label_val, user2_Label_val, user2_Label_val], axis=0)\n",
    "user3_Label_train = np.concatenate([user3_Label_train, user3_Label_train, user3_Label_train, user3_Label_train], axis=0)\n",
    "user3_Label_val = np.concatenate([user3_Label_val, user3_Label_val, user3_Label_val, user3_Label_val], axis=0)\n",
    "\n",
    "Y_train = np.concatenate([Y_train, np.concatenate([user2_Label_train, user3_Label_train], axis=0)], axis=1)\n",
    "Y_val = np.concatenate([Y_val, np.concatenate([user2_Label_val, user3_Label_val], axis=0)], axis=1)\n",
    "\n",
    "Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([user2_Bag_train, user2_Hips_train, user2_Torso_train, user2_Hand_train, user3_Bag_train, user3_Hips_train, user3_Torso_train, user3_Hand_train])\n",
    "X_val = np.concatenate([user2_Bag_val, user2_Hips_val, user2_Torso_val, user2_Hand_val, user3_Bag_val, user3_Hips_val, user3_Torso_val, user3_Hand_val])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, \n",
    "                          n_jobs=-1, tree_method='gpu_hist', gpu_id=0)\n",
    "model.fit(X_train[(Y_train[:, 1] <= 3) & (Y_train[:, 1] > 1)], Y_train[(Y_train[:, 1] <= 3) & (Y_train[:, 1] > 1)][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y,pred_y,class_names,normalize=False):\n",
    "    cm = confusion_matrix(test_y,pred_y)\n",
    "    # classes = class_names[unique_labels(test_y,pred_y)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           ylabel='True label\\n',\n",
    "           xlabel='\\nPredicted label')\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j,\n",
    "                    i,\n",
    "                    format(cm[i, j], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"red\", fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Bag', 'Hips', 'Torso', 'Hand']\n",
    "\n",
    "predict = model.predict(X_val)\n",
    "plot_confusion_matrix(Y_val[(Y_val[:, 1] <= 3) & (Y_val[:, 1] > 1)][:, 0], predict[(Y_val[:, 1] <= 3) & (Y_val[:, 1] > 1)], class_names, True)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test)\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict==i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validationデータをすべて学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_train, X_val], axis=0)\n",
    "Y = np.concatenate([Y_train, Y_val], axis=0)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, \n",
    "                          n_jobs=-1, tree_method='gpu_hist', gpu_id=0)\n",
    "model.fit(X[(Y[:, 1] <= 3) & (Y[:, 1] > 1)], Y[(Y[:, 1] <= 3) & (Y[:, 1] > 1)][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Bag', 'Hips', 'Torso', 'Hand']\n",
    "\n",
    "predict = model.predict(X[(Y[:, 1] <= 3) & (Y[:, 1] > 1)])\n",
    "plot_confusion_matrix(Y[(Y[:, 1] <= 3) & (Y[:, 1] > 1)][:, 0], predict, class_names, True)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test)\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict==i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここで満を持してtestデータの出力確率が75%以上のWalkingかRunだと思われるサンプルだけで保持位置を推定！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"test_walking_run_index.npy\")\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(test[a])\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict[:, i]>=0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 24))\n",
    "xgb.plot_importance(model,\n",
    "                    ax=ax,\n",
    "                    importance_type='gain',\n",
    "                    show_values=False)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainデータで保持位置決定精度を調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Yu Gothic')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(hold_position):\n",
    "    file_path = \"../Output/train/train_\" + hold_position + \"/train_\" + hold_position\n",
    "    xy_mean = np.load(file_path + \"_glo_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_glo_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_glo_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_glo_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_glo_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_glo_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_glo_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_glo_gyro_z_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_glo_mag_norm_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_lacc_FFT = np.load(file_path + \"_glo_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_glo_gyro_z_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_glo_mag_norm_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = load_npy(\"Bag\")\n",
    "Hips = load_npy(\"Hips\")\n",
    "Torso = load_npy(\"Torso\")\n",
    "Hand = load_npy(\"Hand\")\n",
    "\n",
    "Hand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = np.load(\"../Data/センサ別npyファイル/train/train_Bag/train_Bag_Label.npy\")[:, 0].reshape([-1, 1])\n",
    "Label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag_train, Bag_val = train_test_split(Bag, test_size=0.7, shuffle=False)\n",
    "Hips_train, Hips_val = train_test_split(Hips, test_size=0.7, shuffle=False)\n",
    "Torso_train, Torso_val = train_test_split(Torso, test_size=0.7, shuffle=False)\n",
    "Hand_train, Hand_val = train_test_split(Hand, test_size=0.7, shuffle=False)\n",
    "Label_train, Label_val = train_test_split(Label, test_size=0.7, shuffle=False)\n",
    "\n",
    "Hand_train.shape, Hand_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [0] * 58647 + [1] * 58647 + [2] * 58647 + [3] * 58647\n",
    "Y_train = np.array(Y_train).reshape([-1, 1])\n",
    "\n",
    "Y_val = [0] * 136844 + [1] * 136844 + [2] * 136844 + [3] * 136844\n",
    "Y_val = np.array(Y_val).reshape([-1, 1])\n",
    "\n",
    "Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([Bag_train, Hips_train, Torso_train, Hand_train])\n",
    "X_val = np.concatenate([Bag_val, Hips_val, Torso_val, Hand_val])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_train = np.concatenate([Label_train, Label_train, Label_train, Label_train], axis=0)\n",
    "Label_val = np.concatenate([Label_val, Label_val, Label_val, Label_val], axis=0)\n",
    "\n",
    "Label_train.shape, Label_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.concatenate([Y_train, Label_train], axis=1)\n",
    "Y_val = np.concatenate([Y_val, Label_val], axis=1)\n",
    "\n",
    "Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, \n",
    "                          n_jobs=-1, tree_method='gpu_hist', gpu_id=0, n_estimator=50)\n",
    "model.fit(X_train[(Y_train[:, 1] > 1) & (Y_train[:, 1] <= 3)], Y_train[(Y_train[:, 1] > 1) & (Y_train[:, 1] <= 3)][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y,pred_y,class_names,normalize=False):\n",
    "    cm = confusion_matrix(test_y,pred_y)\n",
    "    # classes = class_names[unique_labels(test_y,pred_y)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           ylabel='True label\\n',\n",
    "           xlabel='\\nPredicted label')\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j,\n",
    "                    i,\n",
    "                    format(cm[i, j], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"red\", fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Bag', 'Hips', 'Torso', 'Hand']\n",
    "\n",
    "predict = model.predict(X_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)])\n",
    "plot_confusion_matrix(Y_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)][:, 0], predict, class_names, True)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainデータ全部学習→validationデータの保持位置推定の精度を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from numpy.fft import fft\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Yu Gothic')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = np.load(\"../Data/センサ別npyファイル/train/train_Bag/train_Bag_Label.npy\")[:, 0].reshape([-1])\n",
    "Label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = np.load(\"../Output/train/train_Bag/train_Bag_lacc_norm_spectram.npy\").reshape([-1, 1, 128, 25])\n",
    "Hips = np.delete(np.load(\"../Output/train/train_Hips/train_Hips_lacc_norm_spectram.npy\"), 120845, 0).reshape([-1, 1, 128, 25])\n",
    "Torso = np.load(\"../Output/train/train_Torso/train_Torso_lacc_norm_spectram.npy\").reshape([-1, 1, 128, 25])\n",
    "Hand = np.load(\"../Output/train/train_Hand/train_Hand_lacc_norm_spectram.npy\").reshape([-1, 1, 128, 25])\n",
    "\n",
    "Hand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([Bag, Hips, Torso, Hand], axis=0)\n",
    "Y_train = [0] * 195491 + [1] * 195490 + [2] * 195491 + [3] * 195491\n",
    "Y_train = np.array(Y_train).reshape([-1, 1])\n",
    "\n",
    "Label = np.concatenate([Label, np.delete(Label, 120845, 0), Label, Label], axis=0)\n",
    "\n",
    "Y_train = np.concatenate([Y_train, Label], axis=1)\n",
    "del Label\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy(hold_position):\n",
    "    file_path = \"../Output/validation/validation_\" + hold_position + \"/validation_\" + hold_position\n",
    "    xy_mean = np.load(file_path + \"_glo_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_glo_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_glo_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_glo_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_glo_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_glo_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_glo_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_glo_gyro_z_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_glo_mag_norm_ver2_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_lacc_FFT = np.load(file_path + \"_glo_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_glo_gyro_z_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_glo_mag_norm_ver2_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = load_npy(\"Bag\")\n",
    "Hips = load_npy(\"Hips\")\n",
    "Torso = load_npy(\"Torso\")\n",
    "Hand = load_npy(\"Hand\")\n",
    "\n",
    "Hand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.concatenate([Bag, Hips, Torso, Hand], axis=0)\n",
    "Y_val = [0] * 28685 + [1] * 28685 + [2] * 28685 + [3] * 28685\n",
    "Y_val = np.array(Y_val).reshape([-1, 1])\n",
    "Label = np.load(\"../Data/センサ別npyファイル/validation/validation_Bag/validation_Bag_Label.npy\")[:, 0].reshape([-1, 1])\n",
    "Label = np.concatenate([Label, Label, Label, Label], axis=0)\n",
    "\n",
    "Y_val = np.concatenate([Y_val, Label], axis=1)\n",
    "del Label\n",
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "\n",
    "X_train[(Y_train[:, 1] > 1) | (Y_train[:, 1] <= 3)][:, :39] = std.fit_transform(X_train[(Y_train[:, 1] > 1) | (Y_train[:, 1] <= 3)][:, :39])\n",
    "X_train[(Y_train[:, 1] > 1) | (Y_train[:, 1] <= 3)][:, [i for i in range(40, X_train.shape[1], 2)]] = std.fit_transform(X_train[(Y_train[:, 1] > 1) | (Y_train[:, 1] <= 3)][:, [i for i in range(40, X_train.shape[1], 2)]])\n",
    "\n",
    "X_val[(Y_val[:, 1] > 1) | (Y_val[:, 1] <= 3)][:, :39] = std.fit_transform(X_val[(Y_val[:, 1] > 1) | (Y_val[:, 1] <= 3)][:, :39])\n",
    "X_val[(Y_val[:, 1] > 1) | (Y_val[:, 1] <= 3)][:, [i for i in range(40, X_val.shape[1], 2)]] = std.fit_transform(X_val[(Y_val[:, 1] > 1) | (Y_val[:, 1] <= 3)][:, [i for i in range(40, X_val.shape[1], 2)]])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learns = []\n",
    "tests = []\n",
    "\n",
    "for i in range(4):\n",
    "    tmp = np.where(Y_train[:, 0] == i)[0]\n",
    "    learns.append(tmp[:tmp.shape[0]//10*7])\n",
    "    tests.append(tmp[tmp.shape[0]//10*7:])\n",
    "    \n",
    "learns, tests, len(learns[0]), len(tests[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[learns[0]]\n",
    "y_train = Y_train[learns[0]]\n",
    "x_val = X_train[tests[0]]\n",
    "y_val = Y_train[tests[0]]\n",
    "\n",
    "for i in range(1, len(learns)):\n",
    "    x_train = np.concatenate([x_train, X_train[learns[i]]])\n",
    "    y_train = np.concatenate([y_train, Y_train[learns[i]]])\n",
    "    \n",
    "    x_val = np.concatenate([x_val, X_train[tests[i]]])\n",
    "    y_val = np.concatenate([y_val, Y_train[tests[i]]])\n",
    "    \n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = {}\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, \n",
    "                          n_jobs=-1, tree_method='gpu_hist', gpu_id=0, n_estimator=10000)\n",
    "model.fit(x_train[(y_train[:, 1] > 1) & (y_train[:, 1] <= 3)], y_train[(y_train[:, 1] > 1) & (y_train[:, 1] <= 3)][:, 0], \n",
    "          early_stopping_rounds=30, \n",
    "          eval_set=[(x_train[(y_train[:, 1] > 1) & (y_train[:, 1] <= 3)], y_train[(y_train[:, 1] > 1) & (y_train[:, 1] <= 3)][:, 0]), \n",
    "                    (x_val[(y_val[:, 1] > 1) & (y_val[:, 1] <= 3)], y_val[(y_val[:, 1] > 1) & (y_val[:, 1] <= 3)][:, 0])], eval_metric='merror', verbose=False, \n",
    "          callbacks=[xgb.callback.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習過程の名前は 'validation_{n}' になる\n",
    "plt.figure(figsize=(24, 4))\n",
    "train_metric = evals_result['validation_0']['merror']\n",
    "plt.plot(train_metric, label='train merror')\n",
    "eval_metric = evals_result['validation_1']['merror']\n",
    "plt.plot(eval_metric, label='eval merror')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('rounds(epochs)')\n",
    "plt.ylabel('merror')\n",
    "plt.xticks(np.arange(0, 150+1, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y,pred_y,class_names,normalize=False):\n",
    "    cm = confusion_matrix(test_y,pred_y)\n",
    "    # classes = class_names[unique_labels(test_y,pred_y)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           ylabel='True label\\n',\n",
    "           xlabel='\\nPredicted label')\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j,\n",
    "                    i,\n",
    "                    format(cm[i, j], fmt),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"red\", fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Bag', 'Hips', 'Torso', 'Hand']\n",
    "\n",
    "predict = model.predict(X_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)])\n",
    "plot_confusion_matrix(Y_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)][:, 0], predict, class_names, True)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy():\n",
    "    file_path = \"../Output/test/test\"    \n",
    "    xy_mean = np.load(file_path + \"_laccel_xy_mean.npy\")\n",
    "    xy_var = np.load(file_path + \"_laccel_xy_var.npy\")\n",
    "    z_mean = np.load(file_path + \"_laccel_z_mean.npy\")\n",
    "    z_var = np.load(file_path + \"_laccel_z_var.npy\")\n",
    "    z_skew = np.load(file_path + \"_laccel_z_skew.npy\")\n",
    "    z_kurtosis = np.load(file_path + \"_laccel_z_kurtosis.npy\")\n",
    "    z_lacc_FFT_sum = np.load(file_path + \"_laccel_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_gyro_FFT_sum = np.load(file_path + \"_gyro_z_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    norm_mag_FFT_sum = np.load(file_path + \"_mag_norm_sum_frequency_range5Hz.npy\")[:, 0:-1:2]\n",
    "    z_lacc_FFT = np.load(file_path + \"_laccel_z_amplitude_frequency_range5Hz.npy\")\n",
    "    z_gyro_FFT = np.load(file_path + \"_gyro_z_amplitude_frequency_range5Hz.npy\")\n",
    "    norm_mag_FFT = np.load(file_path + \"_mag_norm_amplitude_frequency_range5Hz.npy\")\n",
    "    result = np.concatenate([xy_mean.reshape([-1, 1]), xy_var.reshape([-1, 1]), z_mean.reshape([-1, 1]), z_var.reshape([-1, 1]), z_skew.reshape([-1, 1]), z_kurtosis.reshape([-1, 1]), \\\n",
    "                             z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_lacc_FFT, z_gyro_FFT, norm_mag_FFT], axis=1)\n",
    "    del xy_mean, xy_var, z_mean, z_var, z_skew, z_kurtosis, z_lacc_FFT_sum, z_gyro_FFT_sum, norm_mag_FFT_sum, z_gyro_FFT, norm_mag_FFT\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_npy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"test_walking_run_index.npy\")\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[a]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:, :39] = std.fit_transform(test[:, :39])\n",
    "test[:, [i for i in range(40, test.shape[1], 2)]] = std.fit_transform(test[:, [i for i in range(40, test.shape[1], 2)]])\n",
    "\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(X_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)])\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict[:, i]>=0.70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(test)\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict[:, i]>=0.75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainデータ全部学習させて、validationデータをvalidationにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = {}\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=18, min_child_weight=7, learning_rate=0.1, gamma=0.005, sub_sample=0.9, colsample_bytree=0.8, \n",
    "                          n_jobs=-1, tree_method='gpu_hist', gpu_id=0, n_estimator=10000)\n",
    "model.fit(X_train[(Y_train[:, 1] > 1) & (Y_train[:, 1] <= 3)], Y_train[(Y_train[:, 1] > 1) & (Y_train[:, 1] <= 3)][:, 0], \n",
    "          early_stopping_rounds=30, \n",
    "          eval_set=[(X_train[(Y_train[:, 1] > 1) & (Y_train[:, 1] <= 3)], Y_train[(Y_train[:, 1] > 1) & (Y_train[:, 1] <= 3)][:, 0]), \n",
    "                    (X_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)], Y_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)][:, 0])], eval_metric='merror', verbose=False, \n",
    "          callbacks=[xgb.callback.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習過程の名前は 'validation_{n}' になる\n",
    "plt.figure(figsize=(24, 4))\n",
    "train_metric = evals_result['validation_0']['merror']\n",
    "plt.plot(train_metric, label='train merror')\n",
    "eval_metric = evals_result['validation_1']['merror']\n",
    "plt.plot(eval_metric, label='eval merror')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('rounds(epochs)')\n",
    "plt.ylabel('merror')\n",
    "plt.xticks(np.arange(0, 150+1, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Bag', 'Hips', 'Torso', 'Hand']\n",
    "\n",
    "predict = model.predict(X_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)])\n",
    "plot_confusion_matrix(Y_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)][:, 0], predict, class_names, True)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(X_val[(Y_val[:, 1] > 1) & (Y_val[:, 1] <= 3)])\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict[:, i]>=0.70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_proba(test)\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print(\"{}：{}\".format(class_names[i], np.sum(predict[:, i]>=0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
